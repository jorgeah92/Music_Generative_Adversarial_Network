{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mido import MidiFile, MidiTrack, Message\n",
    "from music21 import *\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_notes(file_name, song_len=200):\n",
    "    # function to take a midi file and create a dataframe with columns representing note played, duration and time\n",
    "    \n",
    "    # start by reading the file:\n",
    "    message_strings_split = []\n",
    "    mid = MidiFile(file_name) \n",
    "    for i in mid.tracks[1][2:-1]: \n",
    "        message_string = str(i)\n",
    "        message_strings_split.append(message_string.split(\" \"))\n",
    "        \n",
    "    # now extract all the relevant information from the message and create a data frame:\n",
    "    message_type = []\n",
    "    for item in message_strings_split:\n",
    "        message_type.append(item[0])\n",
    "    df1 = pd.DataFrame(message_type)\n",
    "    attributes = []\n",
    "    for item in message_strings_split:\n",
    "        attributes.append(item[1:])\n",
    "    attributes_dict = [{}]    \n",
    "    for item in attributes:\n",
    "        for i in item:\n",
    "            key, val = i.split(\"=\")\n",
    "            if key in attributes_dict[-1]:\n",
    "                attributes_dict.append({})\n",
    "            attributes_dict[-1][key] = val\n",
    "    df2 = pd.DataFrame.from_dict(attributes_dict)\n",
    "    df_complete = pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "    # control change messages are for the pedal...let's simplify by not having those, and don't need all columns:\n",
    "    df_notes = df_complete[df_complete[0] == 'note_on'].drop(columns={0,'channel'}).reset_index(drop=True)\n",
    "    if 'control' in df_notes.columns:\n",
    "        df_notes = df_notes.drop(columns={'control','value'})\n",
    "    \n",
    "    # change some of the data types:\n",
    "    df_notes.time = df_notes.time.astype(float)\n",
    "    df_notes.note = df_notes.note.astype(int)\n",
    "    df_notes.velocity = df_notes.velocity.astype(int)\n",
    "    \n",
    "    # create a time elapsed attribute equal to the cumulative sum of time.\n",
    "    df_notes['time_elapsed'] = df_notes.time.cumsum()\n",
    "    \n",
    "    # say we would work with a small note subset of the song\n",
    "    # for speed we'll just work with the first bit of the song for calculating durations:\n",
    "    df_song_intro = df_notes.loc[0:song_len*3].copy()\n",
    "    \n",
    "    # find the duration each note was played based on the stop note signals (note on with velocity == 0)\n",
    "    duration = [0] * len(df_song_intro)\n",
    "    for i in range(len(df_song_intro)):\n",
    "        if df_notes['velocity'][i] != 0 and i < len(df_song_intro) - 1:            \n",
    "            j = i + 1\n",
    "            while df_song_intro['note'][j] != df_song_intro['note'][i]:\n",
    "                if j >= len(df_song_intro) - 1:\n",
    "                    break\n",
    "                else:\n",
    "                    j += 1\n",
    "            duration[i] = df_song_intro['time_elapsed'][j] - df_song_intro['time_elapsed'][i]\n",
    "    df_song_intro['duration'] = duration\n",
    "\n",
    "    # now drop the \"notes off\" signal rows (this info is in the duration column)\n",
    "    df_song_intro = df_song_intro[df_song_intro['velocity'] != 0].reset_index(drop=True)\n",
    "    # simplify again to start without dynamics:\n",
    "    df_song_intro = df_song_intro.drop(columns={'time','velocity'})\n",
    "    \n",
    "    # now formally take just the first bit of the song:\n",
    "    df_first_notes = df_song_intro.loc[0:song_len-1].copy()\n",
    "    if len(df_first_notes) < song_len:\n",
    "        return np.zeros((1,200,3))\n",
    "    \n",
    "    # now, let's normalize the time elapsed and make duration a fraction of time elapsed:\n",
    "    df_first_notes['time_elapsed'] -= df_first_notes['time_elapsed'][0]\n",
    "    df_first_notes['duration'] /= df_first_notes['time_elapsed'][song_len-1]\n",
    "    df_first_notes['time_elapsed'] /= df_first_notes['time_elapsed'][song_len-1]\n",
    "    \n",
    "    # finally, let's recreate the \"time since last event\" nature of a midi file for time_elapsed:\n",
    "    time_since_last = [0] * song_len\n",
    "    for i in range(1, song_len):\n",
    "        time_since_last[i] = df_first_notes['time_elapsed'][i] - df_first_notes['time_elapsed'][i-1]\n",
    "    df_first_notes['time_since_last'] = time_since_last\n",
    "    df_first_notes = df_first_notes.drop(columns='time_elapsed')\n",
    "    \n",
    "    # lastly, need to normalize the notes...MIDI for piano returns 21 to 108, so:\n",
    "    df_first_notes['note'] -= 20\n",
    "    df_first_notes['note'] /= 88\n",
    "    \n",
    "    return df_first_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_midi(df_first_notes, speed=20000):\n",
    "    # function to take a dataframe created by something like parse_notes() or a gan and return a midi\n",
    "    \n",
    "    # Can start by reverse scaling the note:\n",
    "    df_reversed = df_first_notes.copy()\n",
    "    df_reversed['note'] = round(df_reversed['note'] * 88 + 20)  # might want to have something more special than round()\n",
    "    df_reversed.note = df_reversed.note.astype(int)\n",
    "    df_reversed['velocity'] = 60  # create a uniform middling velocity\n",
    "\n",
    "    # recreate the absolute time index and drop time_since_last (we'll recreate it with the stop signals)\n",
    "    df_reversed['time_index'] = df_reversed.time_since_last.cumsum()\n",
    "    df_reversed = df_reversed.drop(columns = 'time_since_last')\n",
    "\n",
    "    # create a stop signal for each note at the appropriate time_index:\n",
    "    for i in range(len(df_reversed)):\n",
    "        stop_note = pd.DataFrame([[df_reversed.note[i], 0, 0, df_reversed.duration[i] + df_reversed.time_index[i]]],\n",
    "                                 columns=['note', 'duration', 'velocity', 'time_index'])\n",
    "        df_reversed = df_reversed.append(stop_note, ignore_index=True)\n",
    "    df_reversed = df_reversed.sort_values('time_index').reset_index(drop=True)\n",
    "\n",
    "    # recreate time_since last with the stop note signals\n",
    "    df_reversed['time'] = [0] + [df_reversed.time_index[i+1] - df_reversed.time_index[i] \n",
    "                                 for i in range(len(df_reversed)-1)]\n",
    "    # and now we don't need duration or time_index so can drop those\n",
    "    df_reversed = df_reversed.drop(columns = {'time_index','duration'})\n",
    "\n",
    "    # finally, we need to scale the time since last note appropriately:\n",
    "    df_reversed['time'] = round(df_reversed['time'] * speed)\n",
    "    df_reversed.time = df_reversed.time.astype(int)\n",
    "\n",
    "    # finally, recreate the midi and return\n",
    "    mid_remade = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    mid_remade.tracks.append(track)\n",
    "    track.append(Message('program_change', program=0, time=0))\n",
    "    for i in range(len(df_reversed)):\n",
    "        track.append(Message('note_on', note=df_reversed.note[i], velocity=df_reversed.velocity[i], time=df_reversed.time[i]))\n",
    "\n",
    "    return mid_remade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all files into np array:\n",
    "song_len = 200\n",
    "all_songs = np.zeros((1,200,3))  # create a blank first \"song\" to just append things to uniformly in loop\n",
    "\n",
    "for file_name in glob.glob(\"All_Maestro/*.midi\"):\n",
    "    song_notes = parse_notes(file_name, song_len)\n",
    "    if not np.array_equal(song_notes, np.zeros((1,200,3))):\n",
    "        transpose_notes = song_notes.copy()\n",
    "        for i in range(-5,7):\n",
    "            transpose_notes['note'] = song_notes['note'] + i/88\n",
    "            transpose_notes['note'] = [1/88 if i <= 0 else i for i in transpose_notes['note']] # can't go below bottom A\n",
    "            transpose_notes['note'] = [1 if i > 1 else i for i in transpose_notes['note']] # can't go above top C\n",
    "            all_songs = np.append(all_songs, transpose_notes.to_numpy().reshape((1,200,3)), axis=0)\n",
    "\n",
    "all_songs = np.delete(all_songs, 0, 0)  # delete that first blank song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('All_Maestro_Parsed', all_songs) # save the parse data for later use\n",
    "# all_songs = np.load('All_Maestro_Parsed.npy') # test loading of saved array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15300, 200, 3)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can test various elements of array\n",
    "all_songs.shape\n",
    "# all_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test recreating a midi\n",
    "test_song = pd.DataFrame(all_songs[0,:,:].reshape((200,3)), columns=[\"note\", \"duration\",\"time_since_last\"])\n",
    "mid_remade = recreate_midi(test_song, 20000)\n",
    "mid_remade.save('mid_test.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv24776'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv24776');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see if we can play in jupyter notebook\n",
    "mf = midi.MidiFile()\n",
    "mf.open('mid_test.mid')\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = midi.translate.midiFileToStream(mf)\n",
    "s.show('midi') # note at this stage there will be no dynamics and no pedal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
