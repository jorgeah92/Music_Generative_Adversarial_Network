# %%
from __future__ import print_function, division
import sys
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import math
import pickle
import glob
from music21 import *
from keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from keras.layers.advanced_activations import LeakyReLU
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras.utils import np_utils
from mido import MidiFile, MidiTrack, Message

from IPython.display import Image

# Pokemon parse code - START
# %%
# def get_notes():
#     """ Get all the notes and chords from the midi files """
#     notes = []

#     for file in glob.glob("maestro-v3.0.0/test/*.midi"):
#         midi = converter.parse(file)

#         print("Parsing %s" % file)

#         notes_to_parse = None

#         try: # file has instrument parts
#             s2 = instrument.partitionByInstrument(midi)
#             notes_to_parse = s2.parts[0].recurse() 
#         except: # file has notes in a flat structure
#             notes_to_parse = midi.flat.notes
            
#         for element in notes_to_parse:
#             if isinstance(element, note.Note):
#                 notes.append(str(element.pitch))
#             elif isinstance(element, chord.Chord):
#                 notes.append('.'.join(str(n) for n in element.normalOrder))

#     return notes

# # %%
# def prepare_sequences(notes, n_vocab):
#     """ Prepare the sequences used by the Neural Network """
#     sequence_length = 100

#     # Get all pitch names
#     pitchnames = sorted(set(item for item in notes))

#     # Create a dictionary to map pitches to integers
#     note_to_int = dict((note, number) for number, note in enumerate(pitchnames))

#     network_input = []
#     network_output = []

#     # create input sequences and the corresponding outputs
#     for i in range(0, len(notes) - sequence_length, 1):
#         sequence_in = notes[i:i + sequence_length]
#         sequence_out = notes[i + sequence_length]
#         network_input.append([note_to_int[char] for char in sequence_in])
#         network_output.append(note_to_int[sequence_out])

#     n_patterns = len(network_input)

#     # Reshape the input into a format compatible with LSTM layers
#     network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))
    
#     # Normalize input between -1 and 1
#     network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)
#     network_output = np_utils.to_categorical(network_output)

#     return (network_input, network_output)

# # %%
# def generate_notes(model, network_input, n_vocab):
#     """ Generate notes from the neural network based on a sequence of notes """
#     # pick a random sequence from the input as a starting point for the prediction
#     start = numpy.random.randint(0, len(network_input)-1)
    
#     # Get pitch names and store in a dictionary
#     pitchnames = sorted(set(item for item in notes))
#     int_to_note = dict((number, note) for number, note in enumerate(pitchnames))

#     pattern = network_input[start]
#     prediction_output = []

#     # generate 500 notes
#     for note_index in range(500):
#         prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))
#         prediction_input = prediction_input / float(n_vocab)

#         prediction = model.predict(prediction_input, verbose=0)

#         index = numpy.argmax(prediction)
#         result = int_to_note[index]
#         prediction_output.append(result)
        
#         pattern = numpy.append(pattern,index)
#         #pattern.append(index)
#         pattern = pattern[1:len(pattern)]

#     return prediction_output

# # %%  
# def create_midi(prediction_output, filename):
#     """ convert the output from the prediction to notes and create a midi file
#         from the notes """
#     offset = 0
#     output_notes = []

#     # create note and chord objects based on the values generated by the model
#     for item in prediction_output:
#         pattern = item[0]
#         # pattern is a chord
#         if ('.' in pattern) or pattern.isdigit():
#             notes_in_chord = pattern.split('.')
#             notes = []
#             for current_note in notes_in_chord:
#                 new_note = note.Note(int(current_note))
#                 new_note.storedInstrument = instrument.Piano()
#                 notes.append(new_note)
#             new_chord = chord.Chord(notes)
#             new_chord.offset = offset
#             output_notes.append(new_chord)
#         # pattern is a note
#         else:
#             new_note = note.Note(pattern)
#             new_note.offset = offset
#             new_note.storedInstrument = instrument.Piano()
#             output_notes.append(new_note)

#         # increase offset each iteration so that notes do not stack
#         offset += 0.5

#     midi_stream = stream.Stream(output_notes)
#     midi_stream.write('midi', fp='{}.mid'.format(filename))

# END - pokemon parse code

# Luke parse code:

# %%
def parse_notes(file_name, song_len=200):
    """ function to take a midi file and create a dataframe with columns representing note played, duration and time"""
    
    # start by reading the file:
    message_strings_split = []
    mid = MidiFile(file_name) 
    for i in mid.tracks[1][2:-1]: 
        message_string = str(i)
        message_strings_split.append(message_string.split(" "))
        
    # now extract all the relevant information from the message and create a data frame:
    message_type = []
    for item in message_strings_split:
        message_type.append(item[0])
    df1 = pd.DataFrame(message_type)
    attributes = []
    for item in message_strings_split:
        attributes.append(item[1:])
    attributes_dict = [{}]    
    for item in attributes:
        for i in item:
            key, val = i.split("=")
            if key in attributes_dict[-1]:
                attributes_dict.append({})
            attributes_dict[-1][key] = val
    df2 = pd.DataFrame.from_dict(attributes_dict)
    df_complete = pd.concat([df1, df2], axis=1)
    
    # control change messages are for the pedal...let's simplify by not having those, and don't need all columns:
    df_notes = df_complete[df_complete[0] == 'note_on'].drop(columns={0,'channel'}).reset_index(drop=True)
    if 'control' in df_notes.columns:
        df_notes = df_notes.drop(columns={'control','value'})
    
    # change some of the data types:
    df_notes.time = df_notes.time.astype(float)
    df_notes.note = df_notes.note.astype(int)
    df_notes.velocity = df_notes.velocity.astype(int)
    
    # create a time elapsed attribute equal to the cumulative sum of time.
    df_notes['time_elapsed'] = df_notes.time.cumsum()
    
    # say we would work with a small note subset of the song
    # for speed we'll just work with the first bit of the song for calculating durations:
    df_song_intro = df_notes.loc[0:song_len*3].copy()
    
    # find the duration each note was played based on the stop note signals (note on with velocity == 0)
    duration = [0] * len(df_song_intro)
    for i in range(len(df_song_intro)):
        if df_notes['velocity'][i] != 0 and i < len(df_song_intro) - 1:            
            j = i + 1
            while df_song_intro['note'][j] != df_song_intro['note'][i]:
                if j >= len(df_song_intro) - 1:
                    break
                else:
                    j += 1
            duration[i] = df_song_intro['time_elapsed'][j] - df_song_intro['time_elapsed'][i]
    df_song_intro['duration'] = duration

    # now drop the "notes off" signal rows (this info is in the duration column)
    df_song_intro = df_song_intro[df_song_intro['velocity'] != 0].reset_index(drop=True)
    # simplify again to start without dynamics:
    df_song_intro = df_song_intro.drop(columns={'time','velocity'})
    
    # now formally take just the first bit of the song:
    df_first_notes = df_song_intro.loc[0:song_len-1].copy()
    if len(df_first_notes) < song_len:
        return np.zeros((1,200,3))
    
    # now, let's normalize the time elapsed and make duration a fraction of time elapsed:
    df_first_notes['time_elapsed'] -= df_first_notes['time_elapsed'][0]
    df_first_notes['duration'] /= df_first_notes['time_elapsed'][song_len-1]
    df_first_notes['time_elapsed'] /= df_first_notes['time_elapsed'][song_len-1]
    
    # finally, let's recreate the "time since last event" nature of a midi file for time_elapsed:
    time_since_last = [0] * song_len
    for i in range(1, song_len):
        time_since_last[i] = df_first_notes['time_elapsed'][i] - df_first_notes['time_elapsed'][i-1]
    df_first_notes['time_since_last'] = time_since_last
    df_first_notes = df_first_notes.drop(columns='time_elapsed')
    
    # lastly, need to normalize the notes...MIDI for piano returns 21 to 108, so:
    df_first_notes['note'] -= 20
    df_first_notes['note'] /= 88
    
    return df_first_notes   # Returns a [200, 3] matrix 

# %%
def recreate_midi(df_first_notes, speed=20000):
    """ function to take a dataframe created by something like parse_notes() or a gan and return a midi"""
    
    # Can start by reverse scaling the note:
    df_reversed = df_first_notes.copy()
    df_reversed['note'] = round(df_reversed['note'] * 88 + 20)  # might want to have something more special than round()
    df_reversed.note = df_reversed.note.astype(int)
    df_reversed['velocity'] = 60  # create a uniform middling velocity

    # recreate the absolute time index and drop time_since_last (we'll recreate it with the stop signals)
    df_reversed['time_index'] = df_reversed.time_since_last.cumsum()
    df_reversed = df_reversed.drop(columns = 'time_since_last')

    # create a stop signal for each note at the appropriate time_index:
    for i in range(len(df_reversed)):
        stop_note = pd.DataFrame([[df_reversed.note[i], 0, 0, df_reversed.duration[i] + df_reversed.time_index[i]]],
                                 columns=['note', 'duration', 'velocity', 'time_index'])
        df_reversed = df_reversed.append(stop_note, ignore_index=True)
    df_reversed = df_reversed.sort_values('time_index').reset_index(drop=True)

    # recreate time_since last with the stop note signals
    df_reversed['time'] = [0] + [df_reversed.time_index[i+1] - df_reversed.time_index[i] 
                                 for i in range(len(df_reversed)-1)]
    # and now we don't need duration or time_index so can drop those
    df_reversed = df_reversed.drop(columns = {'time_index','duration'})

    # finally, we need to scale the time since last note appropriately:
    df_reversed['time'] = round(df_reversed['time'] * speed)
    df_reversed.time = df_reversed.time.astype(int)

    # finally, recreate the midi and return
    mid_remade = MidiFile()
    track = MidiTrack()
    mid_remade.tracks.append(track)
    track.append(Message('program_change', program=0, time=0))
    for i in range(len(df_reversed)):
        track.append(Message('note_on', note=df_reversed.note[i], velocity=df_reversed.velocity[i], time=df_reversed.time[i]))

    return mid_remade

# %%
class GAN():
    def __init__(self, rows):
        self.seq_length = rows
        self.seq_shape = (self.seq_length, 3)
        self.latent_dim = 1000
        self.disc_loss = []
        self.gen_loss =[]
        
        optimizer = Adam(0.0002, 0.5)

        # Build and compile the discriminator
        self.discriminator = self.build_discriminator()
        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

        # Build the generator
        self.generator = self.build_generator()

        # The generator takes noise as input and generates note sequences
        z = Input(shape=(self.latent_dim,))
        generated_seq = self.generator(z)

        # For the combined model we will only train the generator
        self.discriminator.trainable = False

        # The discriminator takes generated images as input and determines validity
        validity = self.discriminator(generated_seq)

        # The combined model  (stacked generator and discriminator)
        # Trains the generator to fool the discriminator
        self.combined = Model(z, validity)
        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)

    def build_discriminator(self):

        model = Sequential()
        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))
        model.add(Bidirectional(LSTM(512)))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(1, activation='sigmoid'))
        model.summary()

        seq = Input(shape=self.seq_shape)
        validity = model(seq)

        return Model(seq, validity)
      
    def build_generator(self):

        model = Sequential()
        model.add(Dense(256, input_dim=self.latent_dim))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))
        model.add(Reshape(self.seq_shape))
        model.summary()
        
        noise = Input(shape=(self.latent_dim,))
        seq = model(noise)

        return Model(noise, seq)

    def train(self, epochs, batch_size=128, sample_interval=50):

        # Load and convert the data - Pokemon
        # notes = get_notes()
        # n_vocab = len(set(notes))
        # X_train, y_train = prepare_sequences(notes, n_vocab)

        # Parse all files into np array:   *Luke*
        song_len = 200
        all_songs = np.zeros((1,200,3))  # create a blank first "song" to just append things to uniformly in loop

        for file_name in glob.glob("All_Maestro/*.midi"):
            song_notes = parse_notes(file_name, song_len)
            if not np.array_equal(song_notes, np.zeros((1,200,3))):
                transpose_notes = song_notes.copy()
                for i in range(-5,7):
                    transpose_notes['note'] = song_notes['note'] + i/88
                    transpose_notes['note'] = [1/88 if i <= 0 else i for i in transpose_notes['note']] # can't go below bottom A
                    transpose_notes['note'] = [1 if i > 1 else i for i in transpose_notes['note']] # can't go above top C
                    all_songs = np.append(all_songs, transpose_notes.to_numpy().reshape((1,200,3)), axis=0)

        all_songs = np.delete(all_songs, 0, 0)  # delete that first blank song

        # Adversarial ground truths
        real = np.ones((batch_size, 1))
        fake = np.zeros((batch_size, 1))
        
        # Training the model
        for epoch in range(epochs):

            # Training the discriminator
            # Select a random batch of note sequences
            idx = np.random.randint(0, all_songs.shape[0], batch_size)
            real_seqs = all_songs[idx]

            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

            # Generate a batch of new note sequences
            gen_seqs = self.generator.predict(noise)

            # Train the discriminator
            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)
            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)


            #  Training the Generator
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

            # Train the generator (to have the discriminator label samples as real)
            g_loss = self.combined.train_on_batch(noise, real)

            # Print the progress and save into loss lists
            if epoch % sample_interval == 0:
              print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))
              self.disc_loss.append(d_loss[0])
              self.gen_loss.append(g_loss)
        
        self.generate()
        self.plot_loss()
        
    def generate(self):
        # Use random noise to generate sequences
        noise = np.random.normal(0, 1, (1, self.latent_dim))
        predictions = self.generator.predict(noise)
       
        mid_track = recreate_midi(predictions)
        mid_track.save("gan_luke_midi.mid")

    def plot_loss(self):
        plt.plot(self.disc_loss, c='red')
        plt.plot(self.gen_loss, c='blue')
        plt.title("GAN Loss per Epoch")
        plt.legend(['Discriminator', 'Generator'])
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)
        plt.close()

# %%
if __name__ == '__main__':
  gan = GAN(rows=200)    
  gan.train(epochs=5000, batch_size=32, sample_interval=1)
